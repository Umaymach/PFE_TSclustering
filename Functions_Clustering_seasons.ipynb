{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n9rSKl5teER",
        "outputId": "5ebc1d5f-37a1-45d0-ed92-0db718827e0e"
      },
      "outputs": [],
      "source": [
        "%pip install tslearn\n",
        "%pip install netdata_pandas\n",
        "%pip install numpy\n",
        "%pip install yellowbrick\n",
        "%pip install tslearn \n",
        "%pip install sklearn\n",
        "%pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "n84AzwkRFlhV"
      },
      "outputs": [],
      "source": [
        "#All the packages that are used \n",
        "import pandas as pd\n",
        "import requests\n",
        "import csv\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from tslearn.clustering import TimeSeriesKMeans, KShape, KernelKMeans\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tslearn.clustering\n",
        "from tslearn.clustering import TimeSeriesKMeans, KShape, KernelKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tslearn.metrics import dtw\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from datetime import datetime as dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **collecting data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "z_jOmeUq6Zer"
      },
      "outputs": [],
      "source": [
        "#Collecting the data using API by specifing longitude and latitude and saving the file as csv\n",
        "def datacollection(longitude, latitude):\n",
        "    output = r\"\"\n",
        "    url = r\"https://power.larc.nasa.gov/api/temporal/daily/point?start=1981&end=2021&longitude={longitude}&latitude={latitude}&community=ag&parameters=T2M_MAX,T2M_MIN,PRECTOTCORR,QV2M,WS10M&format=csv&header=False\"\n",
        "    request = url.format(longitude=longitude, latitude=latitude)\n",
        "    response = requests.get(url=request, verify=True, timeout=30.00)\n",
        "    open(f'Data_{latitude,longitude}.csv', 'wb').write(response.content)\n",
        "    data= pd.read_csv(f'Data_{latitude,longitude}.csv')\n",
        "    return data\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "RzxSlvU1LGd9"
      },
      "outputs": [],
      "source": [
        "#Preparing the format of the data \n",
        "def formatpreparing(data):\n",
        "    data=data.rename(columns={'YEAR': 'Année'})\n",
        "    data[\"combined\"] = data[\"Année\"]*1000 + data[\"DOY\"]\n",
        "    data[\"Date\"] = pd.to_datetime(data[\"combined\"], format = \"%Y%j\")\n",
        "    data['Jour-Mois']=data['Date'].dt.strftime('%d-%m')\n",
        "    data=data.set_index(\"Date\")\n",
        "    data=data[['Année',\"T2M_MAX\",'T2M_MIN','PRECTOTCORR','QV2M','WS10M','Jour-Mois']]\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "YWilvdgR4AY-"
      },
      "outputs": [],
      "source": [
        "#Organizing the data by the specific culture's season\n",
        "def prepare_data(data, season_debute_month,season_debute_day, season_end_month,season_end_day):\n",
        "    #take only the agricultural season that we working with for the specific culture\n",
        "    data=data[(data.index.month >= season_debute_month) | (data.index.month <= season_end_month)]\n",
        "    data=data[data.index >=dt(1981,season_debute_month,season_debute_day)]\n",
        "    #create the column \"saison agricole\"\n",
        "    years = data[\"Année\"].unique()\n",
        "    data[\"Saison agricole\"] = [0 for i in range(data.shape[0])]\n",
        "    for year in years:\n",
        "        data.loc[(data.index >= dt(year, season_debute_month, season_debute_day)) & (data.index <= dt(year+1, season_end_month, season_end_day)), \"Saison agricole\"] = f\"{year}-{year+1}\"\n",
        "    data=data[[\"T2M_MAX\",'T2M_MIN','PRECTOTCORR','QV2M','WS10M','Saison agricole']]\n",
        "    data=data.reset_index()\n",
        "    #create column \"jour mois\"\n",
        "    data['Jour-Mois']=data[\"Date\"].dt.strftime('%d-%m')\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "HCY4BM393t-1"
      },
      "outputs": [],
      "source": [
        "#Calculating the AGDD and APRE\n",
        "def calculat_AGDD_APRE(data, culture, Tbase):\n",
        "    data[f'GDD de {culture}']=(data[\"T2M_MAX\"]+data[\"T2M_MIN\"])/2-Tbase\n",
        "    data[f'GDD de {culture}'] = data[f'GDD de {culture}'].clip(lower = 0)\n",
        "    data['AGDD'] = data.groupby(['Saison agricole'])[f'GDD de {culture}'].transform(pd.Series.cumsum)\n",
        "    data['APRE'] = data.groupby(['Saison agricole'])['PRECTOTCORR'].transform(pd.Series.cumsum)\n",
        "    data=data[[\"Date\",\"AGDD\",'APRE','QV2M','WS10M','Jour-Mois','Saison agricole']]\n",
        "    data=data.set_index('Saison agricole')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "LxM42IXq1qQY"
      },
      "outputs": [],
      "source": [
        "#Studying the clustering of AGDD\n",
        "def pivot_with_AGDD(data):\n",
        "    cols=data[\"Jour-Mois\"].unique().tolist()\n",
        "    data= data.pivot_table(index=\"Saison agricole\", columns=\"Jour-Mois\", values=\"AGDD\")[cols[:-1]]\n",
        "    data=data.drop('2021-2022')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "LbTo60B6UWuX"
      },
      "outputs": [],
      "source": [
        "#Studying the clustering of APRE\n",
        "def pivot_with_APRE(data):\n",
        "    cols=data[\"Jour-Mois\"].unique().tolist()\n",
        "    data= data.pivot_table(index=\"Saison agricole\", columns=\"Jour-Mois\", values=\"APRE\")[cols[:-1]]\n",
        "    data=data.drop('2021-2022')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "P9dbR_MNVfhq"
      },
      "outputs": [],
      "source": [
        "#Studying the clustering of QV2M\n",
        "def pivot_with_QV2M(data):\n",
        "    cols=data[\"Jour-Mois\"].unique().tolist()\n",
        "    data= data.pivot_table(index=\"Saison agricole\", columns=\"Jour-Mois\", values=\"QV2M\")[cols[:-1]]\n",
        "    data=data.drop('2021-2022')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "J6mfoofWVunT"
      },
      "outputs": [],
      "source": [
        "#Studying the clustering of APRE\n",
        "def pivot_with_WS10M(data):\n",
        "    cols=data[\"Jour-Mois\"].unique().tolist()\n",
        "    data= data.pivot_table(index=\"Saison agricole\", columns=\"Jour-Mois\", values=\"WS10M\")[cols[:-1]]\n",
        "    data=data.drop('2021-2022')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Plotting the seasons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "F55ac8AQTCYq"
      },
      "outputs": [],
      "source": [
        "#Plotting the seasons of each culture\n",
        "def Plotting_seasons(data):\n",
        "    TAGDD=pivot_with_AGDD(data)\n",
        "    TAPRE= pivot_with_APRE(data)\n",
        "    TQV2M=pivot_with_QV2M(data)\n",
        "    TWS10M=pivot_with_WS10M(data)\n",
        "    \n",
        "    for var, i in [(TAGDD,\"AGDD\"),(TAPRE, \"APRE\"),(TQV2M,\"QV2M\"), (TWS10M,\"WS10M\")]:\n",
        "        ax = var.T.plot(figsize=(20, 12))\n",
        "        ax.set_ylabel(i, fontsize=20)\n",
        "        ax.set_xlabel('Jour et mois', fontsize=20)\n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfpSNJ3KaKvf"
      },
      "source": [
        "# **Min max**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "uMd5KbHjVA3q"
      },
      "outputs": [],
      "source": [
        "#Calculating the minmax scaling for the data base\n",
        "def minmax(data):\n",
        "    datanorm = (data-min(data.min()))/(max(data.max())-min(data.min()))\n",
        "    return datanorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWkrwTBLtFVL"
      },
      "source": [
        "## **ELbow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Lle827gMLkd9"
      },
      "outputs": [],
      "source": [
        "#Searching the elbow and the number of the cluters, then predicting \n",
        "def Clustering_with_timeseriesKmeans(data_scaled):\n",
        "    metric_params = {\"global_constraint\":\"sakoe_chiba\", \"sakoe_chiba_radius\": 10}\n",
        "    vis=kelbow_visualizer(TimeSeriesKMeans(random_state=42), data_scaled, k=(2,10), metric_params=metric_params,locate_elbow=True, timings=False, show=True)\n",
        "    vis.fit(data_scaled)\n",
        "    num_K=vis.elbow_value_\n",
        "    models = tslearn.clustering.TimeSeriesKMeans(n_clusters=num_K, metric='dtw',random_state=42, metric_params=metric_params)\n",
        "    predictions = models.fit_predict(data_scaled)\n",
        "    return num_K, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "QcvcbakTsboT"
      },
      "outputs": [],
      "source": [
        "#Display the plots and the clusters by groups\n",
        "def Clusteringplots(data_scaled):\n",
        "    metric_params = {\"global_constraint\":\"sakoe_chiba\", \"sakoe_chiba_radius\": 10}\n",
        "    vis=kelbow_visualizer(TimeSeriesKMeans(random_state=42), data_scaled, k=(2,10), metric_params=metric_params,locate_elbow=True, timings=False, show=True)\n",
        "    vis.fit(data_scaled)\n",
        "    num_K=vis.elbow_value_\n",
        "    # sakoe_chiba_radius=None, itakura_max_slope=None\n",
        "    models = tslearn.clustering.TimeSeriesKMeans(n_clusters=num_K, metric='dtw',random_state=42, metric_params=metric_params)\n",
        "    predictions = models.fit_predict(data_scaled)\n",
        "    plt.figure(figsize=(20,10))\n",
        "\n",
        "    #plt.figure(figsize=(20,10))\n",
        "    X_train = data_scaled.values\n",
        "    for yi in range(4):\n",
        "        plt.subplot(2, 2, yi + 1)\n",
        "        for xx in X_train[predictions == yi]:\n",
        "            _index = T.columns.values\n",
        "            n_indices = _index.shape[0]\n",
        "            _index = [_index[i] for i in range(n_indices) if i%31==0 ]\n",
        "            plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "            plt.xticks(ticks = [i for i in range(n_indices) if  i%31==0], labels = _index)\n",
        "        plt.plot(models.cluster_centers_[yi].ravel(), \"r-\")\n",
        "        plt.xlim(0, X_train.shape[1])\n",
        "        # plt.ylim(-10, 10)\n",
        "        plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "                    transform=plt.gca().transAxes)\n",
        "    data_scaled[\"cluster\"] = predictions\n",
        "\n",
        "    print(f\"the number optimal of classes is:{num_K}\")\n",
        "    print('Cluster 1 :', list(data_scaled[data_scaled.cluster == 0].index))\n",
        "    print('Cluster 2 :', list(data_scaled[data_scaled.cluster == 1].index))\n",
        "    print('Cluster 3 :', list(data_scaled[data_scaled.cluster == 2].index))\n",
        "    print('Cluster 4 :', list(data_scaled[data_scaled.cluster == 3].index))\n",
        "    return  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQrp8bdTPXk"
      },
      "source": [
        "## **Metrics for validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "nvbIcIzsTWm4"
      },
      "outputs": [],
      "source": [
        "#Calculating the silhoute score for the predictions of the last time series clustering \n",
        "def silhouette_score_fct(data_scaled):\n",
        "    predictions=Clustering_with_timeseriesKmeans(data_scaled)\n",
        "    metric_params = {\"global_constraint\":\"sakoe_chiba\", \"sakoe_chiba_radius\": 10}\n",
        "    s=tslearn.clustering.silhouette_score(data_scaled,predictions[1], metric=\"dtw\",random_state=5, metric_params=metric_params) \n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "AQeZh9vnUVJN"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "#Calculating the calinski harabasz score for the predictions of the last time series clustering \n",
        "def calinski_harabasz_score(data_scaled):\n",
        "    predictions=Clustering_with_timeseriesKmeans(data_scaled)\n",
        "    metric_params = {\"global_constraint\":\"sakoe_chiba\", \"sakoe_chiba_radius\": 10}\n",
        "    s=sklearn.metrics.calinski_harabasz_score(data_scaled,predictions[1] )\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bpHlpntoDLi"
      },
      "source": [
        "## **Calculating the means**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "-yzi6IySggbV"
      },
      "outputs": [],
      "source": [
        "#Calculating the means that going to be used in the second clustering for AGDD\n",
        "def means_AGDD():\n",
        "    pd.set_option('max_columns', 6)\n",
        "    predictionsT=Clustering_with_timeseriesKmeans(minmax(pivot_with_AGDD(d)))\n",
        "    #predictionsP=Clustering_with_timeseriesKmeans(minmax(pivot_with_APRE(d)))\n",
        "\n",
        "    X=pivot_with_AGDD(d)\n",
        "    X[\"cluster\"] = predictionsT[1]\n",
        "\n",
        "    Tcarac = {i: X[X.cluster == i] for i in X.cluster}\n",
        "    for i in X.cluster :\n",
        "        Tcarac[i]['Max']=Tcarac[i].iloc[:, 0:366].max(axis=1)  \n",
        "    agdd_means=[]\n",
        "    for i in X.cluster:\n",
        "        agdd_means.append(Tcarac[i][['Max']].mean(axis=0).item())\n",
        "    \n",
        "    agdd_means=pd.DataFrame(agdd_means,columns = ['La moyenne de l\\'AGDD pour chaque cluster'],index=X.index)\n",
        "    agdd_means=agdd_means.join(X[[\"cluster\"]])\n",
        "    \n",
        "    return agdd_means\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cmYu8F6sxyYj"
      },
      "outputs": [],
      "source": [
        "#Calculating the means that going to be used in the second clustering for APRE\n",
        "def means_APRE():\n",
        "    pd.set_option('max_columns', 6)\n",
        "    predictionsP=Clustering_with_timeseriesKmeans(minmax(pivot_with_APRE(d)))\n",
        "\n",
        "    Y=pivot_with_APRE(d)\n",
        "    Y[\"cluster\"] = predictionsP[1]\n",
        "\n",
        "    Pcarac = {i: Y[Y.cluster == i] for i in Y.cluster}\n",
        "    for i in Y.cluster :\n",
        "        Pcarac[i]['Max']=Pcarac[i].iloc[:, 0:366].max(axis=1)  \n",
        "    apre_means=[]\n",
        "    for i in Y.cluster:\n",
        "        apre_means.append(Pcarac[i][['Max']].mean(axis=0).item())\n",
        "    \n",
        "    apre_means=pd.DataFrame(apre_means,columns = ['La moyenne de l\\'APRE pour chaque cluster'],index=Y.index)\n",
        "    apre_means=apre_means.join(Y[[\"cluster\"]])\n",
        "    \n",
        "    return apre_means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "9RGPRKUyyz01"
      },
      "outputs": [],
      "source": [
        "#Grouping the means for the final data\n",
        "def final_data():\n",
        "    agdd_means=means_AGDD()\n",
        "    apre_means=means_APRE()\n",
        "    Final_data=pd.DataFrame(agdd_means[\"La moyenne de l'AGDD pour chaque cluster\"])\n",
        "    Final_data[\"La moyenne de l'APRE pour chaque cluster\"]=apre_means[\"La moyenne de l'APRE pour chaque cluster\"]\n",
        "    return Final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8z2HJ3F1OPg"
      },
      "source": [
        "## Final clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "5bcNUNNtpIaD"
      },
      "outputs": [],
      "source": [
        "# Scaling the final data fpr the second clustering\n",
        "def minmaxscaling_final():\n",
        "    scaler = MinMaxScaler()\n",
        "    Final_data=final_data()\n",
        "    col = Final_data.columns\n",
        "    dfscaled = scaler.fit_transform(Final_data)\n",
        "    dfscaled = pd.DataFrame(dfscaled,columns=col, index=Final_data.index)\n",
        "    return dfscaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "LOW_hC6npWTP"
      },
      "outputs": [],
      "source": [
        "# The final clustering and the result of the groups\n",
        "def finalclustering():\n",
        "    dfscaled=minmaxscaling_final()\n",
        "    Final_data=final_data()\n",
        "    visualizer=kelbow_visualizer(KMeans(random_state=42), dfscaled, k=(2,10),locate_elbow=True, timings=False)\n",
        "    NC= visualizer.elbow_value_\n",
        "    models = KMeans(n_clusters=NC,random_state=42)\n",
        "    predictions = models.fit_predict(Final_data)\n",
        "    print(predictions)\n",
        "    Final_data[\"Final_Clustering\"]=models.labels_\n",
        "    print('Cluster 1 :', list(Final_data[Final_data.Final_Clustering == 0].index))\n",
        "    print('Cluster 2 :', list(Final_data[Final_data.Final_Clustering == 1].index))\n",
        "    print('Cluster 3 :', list(Final_data[Final_data.Final_Clustering == 2].index))\n",
        "    print('Cluster 4 :', list(Final_data[Final_data.Final_Clustering == 3].index))\n",
        "    print('Cluster 5 :', list(Final_data[Final_data.Final_Clustering == 4].index))\n",
        "    return\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "a697275c0575815e70aa8d133a025a1cd94b2c6b1af7d3ebd4abef4f1ea97d28"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
